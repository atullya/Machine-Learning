{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f9d1678b-18e4-4395-acb7-ff641bba8b8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import random\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ec5dd310-a448-4aa7-afef-82d411469375",
   "metadata": {},
   "outputs": [],
   "source": [
    "text='''\n",
    "Lorem Ipsum is simply dummy text of the printing and typesetting industry. \n",
    "Lorem Ipsum has been the industry's standard dummy text ever since the 1500s, when an unknown printer took a galley of type and scrambled it to make a type specimen book. It has survived not only five centuries, but also the leap into electronic typesetting, remaining essentially unchanged. It was popularised in the 1960s with the release of Letraset \n",
    "sheets containing Lorem Ipsum passages, and more recently with desktop publishing software like Aldus PageMaker including versions of Lorem Ipsum.\n",
    "'''\n",
    "num_questions=5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "01dcf92e-ae03-4076-a3f8-36303f8ce86a",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp=spacy.load('en_core_web_sm')\n",
    "# spacy.load('en_core_web_sm')\n",
    "\n",
    "# Loads a pre-trained English NLP model (in this case, the small model).\n",
    "\n",
    "# 'en_core_web_sm' means:\n",
    "\n",
    "# en = English language\n",
    "\n",
    "# core = core features of spaCy\n",
    "\n",
    "# web = trained on web data\n",
    "\n",
    "# sm = small size (faster, but less accurate than md or lg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "16afcb4c-a9d9-4969-8062-755d288f3b64",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nLorem Ipsum is simply dummy text of the printing and typesetting industry. \\nLorem Ipsum has been the industry's standard dummy text ever since the 1500s, when an unknown printer took a galley of type and scrambled it to make a type specimen book. It has survived not only five centuries, but also the leap into electronic typesetting, remaining essentially unchanged. It was popularised in the 1960s with the release of Letraset \\nsheets containing Lorem Ipsum passages, and more recently with desktop publishing software like Aldus PageMaker including versions of Lorem Ipsum.\\n\""
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "71f8ce1e-fdba-4826-ae97-822eadcf0534",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\n",
       " Lorem Ipsum is simply dummy text of the printing and typesetting industry. ,\n",
       " Lorem Ipsum has been the industry's standard dummy text ever since the 1500s, when an unknown printer took a galley of type and scrambled it to make a type specimen book.,\n",
       " It has survived not only five centuries, but also the leap into electronic typesetting, remaining essentially unchanged.,\n",
       " It was popularised in the 1960s with the release of Letraset \n",
       " sheets containing Lorem Ipsum passages, and more recently with desktop publishing software like Aldus PageMaker including versions of Lorem Ipsum.]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc=nlp(text)\n",
    "[sent for sent in doc.sents]\n",
    "\n",
    "# 👉 doc = nlp(text)\n",
    "\n",
    "# This processes your text using the spaCy pipeline (tokenization, tagging, parsing, etc.).\n",
    "\n",
    "# doc is a spaCy Doc object that contains the processed text.\n",
    "\n",
    "# 👉 [sent for sent in doc.sents]\n",
    "\n",
    "# This is a list comprehension.\n",
    "\n",
    "# doc.sents is a generator that gives you each sentence (a Span object) in the Doc.\n",
    "\n",
    "# The code collects each sentence into a Python list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fad7552a-fc02-4d7f-8202-84965b1e131a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "len([sent.text for sent in doc.sents])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d6475b34-d1d8-42eb-b373-a411aca439a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['\\nLorem Ipsum is simply dummy text of the printing and typesetting industry. \\n',\n",
       " \"Lorem Ipsum has been the industry's standard dummy text ever since the 1500s, when an unknown printer took a galley of type and scrambled it to make a type specimen book.\",\n",
       " 'It has survived not only five centuries, but also the leap into electronic typesetting, remaining essentially unchanged.',\n",
       " 'It was popularised in the 1960s with the release of Letraset \\nsheets containing Lorem Ipsum passages, and more recently with desktop publishing software like Aldus PageMaker including versions of Lorem Ipsum.\\n']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Extract sentences from the text\n",
    "sentences=[sent.text for sent in doc.sents]\n",
    "sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4fbbd6fe-b26a-4194-ad68-0877f8aecf60",
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_sentences=random.sample(sentences,(min(num_questions,len(sentences))))\n",
    "# 👉 sentences\n",
    "# A list of sentences (for example, from [sent for sent in doc.sents]).\n",
    "\n",
    "# 👉 num_questions\n",
    "# A variable that probably defines how many MCQs (or sentences) you want to select.\n",
    "\n",
    "# 👉 len(sentences)\n",
    "# Total number of sentences available.\n",
    "\n",
    "# 👉 min(num_questions, len(sentences))\n",
    "# This ensures you don’t try to pick more sentences than exist.\n",
    "# It chooses the smaller of:\n",
    "\n",
    "# num_questions (how many you want)\n",
    "\n",
    "# len(sentences) (how many are available)\n",
    "\n",
    "# 👉 random.sample(sentences, ...)\n",
    "# This randomly selects that many sentences from the sentences list without replacement (no duplicates).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "02f1c73a-4fa3-464a-8a8f-4a1730297abe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['It has survived not only five centuries, but also the leap into electronic typesetting, remaining essentially unchanged.',\n",
       " \"Lorem Ipsum has been the industry's standard dummy text ever since the 1500s, when an unknown printer took a galley of type and scrambled it to make a type specimen book.\",\n",
       " 'It was popularised in the 1960s with the release of Letraset \\nsheets containing Lorem Ipsum passages, and more recently with desktop publishing software like Aldus PageMaker including versions of Lorem Ipsum.\\n',\n",
       " '\\nLorem Ipsum is simply dummy text of the printing and typesetting industry. \\n']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "selected_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e662c371-cb56-4579-8552-b859a58f5eff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['leap', 'leap', 'leap', 'centuries']\n",
      "['galley', 'book', 'printer', 'type']\n",
      "['passages', 'publishing', 'ipsum', 'release']\n",
      "['industry', 'text', 'industry', 'industry']\n"
     ]
    }
   ],
   "source": [
    "mcq=[]\n",
    "\n",
    "for sentence in selected_sentences:\n",
    "    sentence=sentence.lower()\n",
    "    #process with spacy(sentences)\n",
    "    sent_doc=nlp(sentence)\n",
    "    # print(sentence)\n",
    "\n",
    "    #extract entites(noun) from the sentence\n",
    "    nouns=[token.text for token in sent_doc if token.pos_==\"NOUN\"]\n",
    "    # print(nouns)\n",
    "\n",
    "    if len(nouns)<2:\n",
    "        continue\n",
    "\n",
    "    nouns_counts=Counter(nouns)\n",
    "    # print(nouns_counts)\n",
    "\n",
    "    if nouns_counts:\n",
    "        subject=nouns_counts.most_common(1)[0][0]\n",
    "        answer_choices=[subject]\n",
    "        # print(answer_counts)\n",
    "        question_steam=sentence.replace(subject,\"____________\")\n",
    "        # print(question_steam)\n",
    "\n",
    "    for _ in range(3):\n",
    "        distractor=random.choice(list(set(nouns)-set([subject])))\n",
    "        answer_choices.append(distractor)\n",
    "\n",
    "    random.shuffle(answer_choices)\n",
    "    print(answer_choices)\n",
    "\n",
    "    correct_answer=chr(64+ answer_choices.index(subject)+1)\n",
    "    mcq.append((question_steam,answer_choices,correct_answer))\n",
    "        # print(distractor)\n",
    "    \n",
    "\n",
    "#     👉 mcq = []\n",
    "# ➡ Creates an empty list that will probably store your generated MCQs later.\n",
    "\n",
    "# 👉 for sentence in selected_sentences:\n",
    "# ➡ Loops through each sentence you randomly selected earlier.\n",
    "\n",
    "# 👉 sent_doc = nlp(sentence)\n",
    "# ➡ Processes each sentence using the spaCy pipeline (nlp).\n",
    "# ➡ This gives you sent_doc, a spaCy Doc object where the sentence is tokenized, tagged, etc.\n",
    "\n",
    "# 👉 print(sentence)\n",
    "# ➡ Prints the original sentence to the console (for debugging or checking what’s being processed).\n",
    "\n",
    "# 👉 nouns = [token.text for token in sent_doc if token.pos_ == \"NOUN\"]\n",
    "# ➡ List comprehension:\n",
    "\n",
    "# Loops through each token (word, punctuation, etc.) in sent_doc.\n",
    "\n",
    "# Checks if the part-of-speech tag (pos_) is \"NOUN\" (common noun).\n",
    "\n",
    "# Collects the text of each noun into the nouns list.\n",
    "\n",
    "# ➡ In short: Extracts all the nouns from the sentence.\n",
    "\n",
    "# 👉 print(nouns)\n",
    "# ➡ Prints the list of nouns found in that sentence.\n",
    "\n",
    "\n",
    "#IN SUMMARY\n",
    "\n",
    "# 👉 For each selected sentence:\n",
    "# 1️⃣ Convert to lowercase → This ensures consistent text (e.g., “City” and “city” are treated the same).\n",
    "# 2️⃣ Process with spaCy (nlp) → Breaks the sentence into tokens and tags their parts of speech.\n",
    "# 3️⃣ Extract nouns → Creates a list of all the words that are tagged as nouns.\n",
    "# 4️⃣ Print the noun list → Shows which nouns were found.\n",
    "# 5️⃣ If fewer than 2 nouns → skip → The sentence is ignored if it doesn’t have enough nouns (likely not useful for an MCQ).\n",
    "# 6️⃣ Count how often each noun appears → Builds a Counter (a dictionary-like object that counts occurrences of each noun).\n",
    "# 7️⃣ Print noun counts → Displays how many times each noun appears in the sentence.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c2cb82d1-230c-4ade-86c0-6c35f7081cc3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('it has survived not only five ____________, but also the leap into electronic typesetting, remaining essentially unchanged.',\n",
       "  ['leap', 'leap', 'leap', 'centuries'],\n",
       "  'D'),\n",
       " (\"lorem ipsum has been the industry's standard dummy text ever since the 1500s, when an unknown printer took a galley of ____________ and scrambled it to make a ____________ specimen book.\",\n",
       "  ['galley', 'book', 'printer', 'type'],\n",
       "  'D'),\n",
       " ('it was popularised in the 1960s with the release of letraset \\nsheets containing lorem ____________ passages, and more recently with desktop publishing software like aldus pagemaker including versions of lorem ____________.\\n',\n",
       "  ['passages', 'publishing', 'ipsum', 'release'],\n",
       "  'C'),\n",
       " ('\\nlorem ipsum is simply dummy ____________ of the printing and typesetting industry. \\n',\n",
       "  ['industry', 'text', 'industry', 'industry'],\n",
       "  'B')]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mcq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f4b59297-f13a-4d31-9081-055e46f07669",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from collections import Counter\n",
    "import random\n",
    "\n",
    "# Load English tokenizer, tagger, parser, NER, and word vectors\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "def generate_mcqs(text, num_questions=5):\n",
    "    # Process the text with spaCy\n",
    "    doc = nlp(text)\n",
    "\n",
    "    # Extract sentences from the text\n",
    "    sentences = [sent.text for sent in doc.sents]\n",
    "\n",
    "    # Randomly select sentences to form questions\n",
    "    selected_sentences = random.sample(sentences, min(num_questions, len(sentences)))\n",
    "\n",
    "    # Initialize list to store generated MCQs\n",
    "    mcqs = []\n",
    "\n",
    "    # Generate MCQs for each selected sentence\n",
    "    for sentence in selected_sentences:\n",
    "        # Process the sentence with spaCy\n",
    "        sent_doc = nlp(sentence)\n",
    "\n",
    "        # Extract entities (nouns) from the sentence\n",
    "        nouns = [token.text for token in sent_doc if token.pos_ == \"NOUN\"]\n",
    "\n",
    "        # Ensure there are enough nouns to generate MCQs\n",
    "        if len(nouns) < 2:\n",
    "            continue\n",
    "\n",
    "        # Count the occurrence of each noun\n",
    "        noun_counts = Counter(nouns)\n",
    "\n",
    "        # Select the most common noun as the subject of the question\n",
    "        if noun_counts:\n",
    "            subject = noun_counts.most_common(1)[0][0]\n",
    "\n",
    "            # Generate the question stem\n",
    "            question_stem = sentence.replace(subject, \"_______\")\n",
    "\n",
    "            # Generate answer choices\n",
    "            answer_choices = [subject]\n",
    "\n",
    "            # Add some random words from the text as distractors\n",
    "            for _ in range(3):\n",
    "                distractor = random.choice(list(set(nouns) - set([subject])))\n",
    "                answer_choices.append(distractor)\n",
    "\n",
    "            # Shuffle the answer choices\n",
    "            random.shuffle(answer_choices)\n",
    "\n",
    "            # Append the generated MCQ to the list\n",
    "            correct_answer = chr(64 + answer_choices.index(subject) + 1)  # Convert index to letter\n",
    "            mcqs.append((question_stem, answer_choices, correct_answer))\n",
    "\n",
    "    return mcqs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "76e8dfe9-0a8f-457c-9986-478d05f8fa32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q1: \n",
      "_______ is a modern web framework that is relatively fast and used for building APIs with Python 3.7+ based on standard Python-type hints.\n",
      "A: APIs\n",
      "B: web\n",
      "C: FastAPI\n",
      "D: type\n",
      "Correct Answer: C\n",
      "Q2: FastAPI also assists us in automatically producing _______ for our web service so that other developers can quickly understand how to use it.\n",
      "A: service\n",
      "B: web\n",
      "C: documentation\n",
      "D: web\n",
      "Correct Answer: C\n",
      "Q3: Unlike the Tigris and Euphrates, the Nile River flooded at the same _______ every year, so farmers could predict when to plant their crops.\n",
      "\n",
      "A: crops\n",
      "B: crops\n",
      "C: year\n",
      "D: time\n",
      "Correct Answer: D\n",
      "Q4: FastAPI is fully compatible with well-known _______ of APIslong the Nile’s shores.\n",
      "A: standards\n",
      "B: shores\n",
      "C: shores\n",
      "D: shores\n",
      "Correct Answer: A\n",
      "Q5: This _______ simplifies testing web service to understand what data it requires and what it offers.\n",
      "A: testing\n",
      "B: web\n",
      "C: documentation\n",
      "D: simplifies\n",
      "Correct Answer: C\n",
      "Q6: FastAPI has many _______ like it offers significant speed for development and also reduces human errors in the code.\n",
      "A: development\n",
      "B: features\n",
      "C: code\n",
      "D: code\n",
      "Correct Answer: B\n"
     ]
    }
   ],
   "source": [
    "# Test the function with the provided text\n",
    "text = \"\"\"\n",
    "FastAPI is a modern web framework that is relatively fast and used for building APIs with Python 3.7+ based on standard Python-type hints. FastAPI also assists us in automatically producing documentation for our web service so that other developers can quickly understand how to use it. This documentation simplifies testing web service to understand what data it requires and what it offers. FastAPI has many features like it offers significant speed for development and also reduces human errors in the code. It is easy to learn and is completely production-ready. FastAPI is fully compatible with well-known standards of APIslong the Nile’s shores. This soil was fertile, which means it was good for growing crops. Unlike the Tigris and Euphrates, the Nile River flooded at the same time every year, so farmers could predict when to plant their crops.\n",
    "\"\"\"\n",
    "\n",
    "results = generate_mcqs(text, num_questions=7)\n",
    "\n",
    "\n",
    "for i, mcq in enumerate(results,start=1):\n",
    "    question_stem, answer_choices, correct_answer = mcq\n",
    "    \n",
    "    print(f\"Q{i}: {question_stem}\")\n",
    "    for j, choice  in enumerate(answer_choices, start=1):\n",
    "        print(f\"{chr(64+j)}: {choice}\")\n",
    "    print(f\"Correct Answer: {correct_answer}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f43cf66-39fa-4da8-bc97-17bd0c7e4f72",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from nltk.corpus import wordnet\n",
    "import random\n",
    "\n",
    "# Load spaCy English model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "def generate_mcqs(text, num_mcqs=5):\n",
    "    doc = nlp(text)\n",
    "    sentences = [sent.text for sent in doc.sents]\n",
    "    mcqs = []\n",
    "\n",
    "    random.shuffle(sentences)  # Shuffle sentences to pick randomly\n",
    "\n",
    "    for sentence in sentences:\n",
    "        sent_doc = nlp(sentence.lower())\n",
    "        # Extract nouns or proper nouns\n",
    "        candidates = [token.text for token in sent_doc if token.pos_ in [\"NOUN\", \"PROPN\"]]\n",
    "\n",
    "        if not candidates:\n",
    "            continue\n",
    "\n",
    "        # Choose a target word (noun/proper noun) to blank\n",
    "        answer = random.choice(candidates)\n",
    "\n",
    "        # Formulate question\n",
    "        question = sentence.replace(answer, \"_______\", 1)\n",
    "\n",
    "        # Generate distractors\n",
    "        distractors = get_distractors(answer)\n",
    "        \n",
    "        # If not enough distractors, pad with random general words\n",
    "        while len(distractors) < 3:\n",
    "            filler = random.choice([\"city\", \"country\", \"river\", \"building\", \"continent\", \"company\"])\n",
    "            if filler != answer and filler not in distractors:\n",
    "                distractors.append(filler)\n",
    "\n",
    "        # Form options\n",
    "        options = distractors + [answer]\n",
    "        random.shuffle(options)\n",
    "\n",
    "        mcqs.append({\n",
    "            \"question\": question,\n",
    "            \"answer\": answer,\n",
    "            \"options\": options\n",
    "        })\n",
    "\n",
    "        if len(mcqs) >= num_mcqs:\n",
    "            break\n",
    "\n",
    "    return mcqs\n",
    "\n",
    "def get_distractors(word):\n",
    "    distractors = []\n",
    "    synsets = wordnet.synsets(word)\n",
    "    if synsets:\n",
    "        for lemma in synsets[0].lemmas():\n",
    "            option = lemma.name().replace('_', ' ')\n",
    "            if option.lower() != word.lower() and option not in distractors:\n",
    "                distractors.append(option)\n",
    "            if len(distractors) >= 3:\n",
    "                break\n",
    "    return distractors\n",
    "\n",
    "# --- Example usage ---\n",
    "input_text = input(\"Enter your text: \")\n",
    "mcqs = generate_mcqs(input_text)\n",
    "\n",
    "# Display MCQs\n",
    "for i, mcq in enumerate(mcqs, 1):\n",
    "    print(f\"\\nQ{i}: {mcq['question']}\")\n",
    "    for opt in mcq['options']:\n",
    "        print(f\"- {opt}\")\n",
    "    print(f\"(Correct answer: {mcq['answer']})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10dff1b3-0c2b-4c21-92e9-9c3f68380481",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
